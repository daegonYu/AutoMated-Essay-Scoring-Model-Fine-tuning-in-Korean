경로 : .vscode , asap.ini

배치사이즈, 에폭, 모델 저장한 models 폴더


models 1,2,3
에폭 20 실험 
weight_decay = 0.05
bias : 7
loss : a=2;b=1;c=1

models 1,2,3 이어서 실험
60 에폭 => 해당 실험 결과가 models/ 4,5,6  
bias : 7
weight_decay = 0.01 
loss : a=2;b=1;c=1

models 7,8,9
bias : 2
weight_decay = 0.05
loss : a=1;b=1;c=1

models 10,11,12 -> models/ 4,5,6 에서 epochs 20 추가 진행
weight_decay = 0.05
loss : a=2;b=1;c=1

2/13
13~
bigbird 램 용량 부족
bigbird 사용을 위해 max_input_length 다 4096으로 변경함 (3군데)
+ 모델, 토크나이저 bigbird로 변경
배치사이즈 1로 변경

2/13
klue bert로 돌려놈

models 13~ : 풍부함 모델 학습


고려사항

* kobigbird : transformers.BigBirdConfig(max_position_embeddings=1024) -> 브랜치 나누기 아니면 코드에 KLUE_BERT인지 BigBird인지 나눠서 돌아가게끔 하면 되는데 건들게 많다. -> 브랜치 나누자.

1. AdamW / Adam + lr scheduler로 바꿔보기 -> RAdam으로 바꿔보기 // 에폭 2단위로 나눠서 validation -> loss, pearson, qwk 그래프 확인하기
: AdamW는 여기서 local optima에 걸리는듯? 나중에 전부 3으로 예측해버린다..
: Adam + lr scheduler는 나름 잘 학습한다. lr scheduler가 있어야 lr이 점점 작아지면서 optima에 도달할 수 있다.

: RAdam은 lr을 자동으로 찾아주는 역할을 한다. 원래 lr warmup을 통해 했었다. 이건 휴리스틱하게 정해줘야 되는 불편함이 있는데 그것을 개선한 것이다.
+ 그럼 RAdam에 lr scheduler를 추가해야하나? -> No! RAdam의 Loss 그래프를 보니 lr 스케줄러는 필요없을 듯 하다.
그래프르르 보니 RAdam이나 Adam + lr 스케줄러가 잘 학습되는 것 같다. 

Adam + lr 스케줄러하고 전에 했던 거 하고 성능 비교해보기

2. 로스 함수 계수 변경해보며 성능평가하기 -> 하이퍼파라미터 찾기

3. 엑셀 방사형 차트 그리기(matplotlib radar chart)

4. 5 fold validation 적용하기 16 epoch씩 5번 총 80에폭

5. 최고성능(모델이 저장될 때)만 result.txt 저장하는 식으로 코드 변경하기 

6. pearson 하고 qwk 평가지표 의미 알아보기
: pearson이 높다는 건 정답 값이 올라갈 땐 올라가고 내려갈 땐 내려가는데 qwk가 높아야 4점이 4점다운 점수가 나오고 2점이 2점다운 점수가 나온다.
즉, 어느정도의 pearson이 확보되면  qwk가 좀 더 중요하다.